{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "REVISED Data Analyst Internship Submission.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ammar0211/Mini-Projects/blob/master/REVISED_Data_Analyst_Internship_Submission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9ZGuRVBB6a4",
        "colab_type": "text"
      },
      "source": [
        "# REVIEW SENTIMENT ANALYSIS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4wPsLGfJwSf",
        "colab_type": "text"
      },
      "source": [
        "INSTALLING LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "0sq1ZcxXB6Z6",
        "colab_type": "code",
        "outputId": "80704501-982a-4a97-dee5-45b5bb72c119",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install selenium #installing selenium for all reviews fetching\n",
        "!apt-get update # to update ubuntu to correctly run apt install\n",
        "!apt install chromium-chromedriver #installing chrome driver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin #copying driver to current directory "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting selenium\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
            "\r\u001b[K     |▍                               | 10kB 14.8MB/s eta 0:00:01\r\u001b[K     |▊                               | 20kB 3.0MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 4.3MB/s eta 0:00:01\r\u001b[K     |█▌                              | 40kB 2.8MB/s eta 0:00:01\r\u001b[K     |█▉                              | 51kB 3.5MB/s eta 0:00:01\r\u001b[K     |██▏                             | 61kB 4.1MB/s eta 0:00:01\r\u001b[K     |██▌                             | 71kB 4.7MB/s eta 0:00:01\r\u001b[K     |███                             | 81kB 5.3MB/s eta 0:00:01\r\u001b[K     |███▎                            | 92kB 5.9MB/s eta 0:00:01\r\u001b[K     |███▋                            | 102kB 4.6MB/s eta 0:00:01\r\u001b[K     |████                            | 112kB 4.6MB/s eta 0:00:01\r\u001b[K     |████▍                           | 122kB 4.6MB/s eta 0:00:01\r\u001b[K     |████▊                           | 133kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 143kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 153kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 163kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 174kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 184kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 194kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 204kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 215kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 225kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 235kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 245kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████                       | 256kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 266kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 276kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 286kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 296kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 307kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 317kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 327kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 337kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 348kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 358kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 368kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 378kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 389kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 399kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 409kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 419kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 430kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 440kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████                | 450kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 460kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 471kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 481kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 491kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 501kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 512kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 522kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 532kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 542kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 552kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 563kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 573kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 583kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 593kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 604kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 614kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 624kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 634kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 645kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 655kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 665kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 675kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 686kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 696kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 706kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 716kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 727kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 737kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 747kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 757kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 768kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 778kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 788kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 798kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 808kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 819kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 829kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 839kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 849kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 860kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 870kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 880kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 890kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 901kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 911kB 4.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3)\n",
            "Installing collected packages: selenium\n",
            "Successfully installed selenium-3.141.0\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Get:5 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:7 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:11 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [794 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [759 kB]\n",
            "Get:16 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [34.2 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [6,783 B]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [18.9 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [10.5 kB]\n",
            "Get:20 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,741 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1,056 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,321 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [32.7 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [4,244 B]\n",
            "Get:25 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [840 kB]\n",
            "Fetched 6,911 kB in 4s (1,584 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following packages were automatically installed and are no longer required:\n",
            "  cuda-cufft-10-1 cuda-cufft-dev-10-1 cuda-curand-10-1 cuda-curand-dev-10-1\n",
            "  cuda-cusolver-10-1 cuda-cusolver-dev-10-1 cuda-cusparse-10-1\n",
            "  cuda-cusparse-dev-10-1 cuda-drivers cuda-license-10-2 cuda-npp-10-1\n",
            "  cuda-npp-dev-10-1 cuda-nsight-10-1 cuda-nsight-compute-10-1\n",
            "  cuda-nsight-systems-10-1 cuda-nvgraph-10-1 cuda-nvgraph-dev-10-1\n",
            "  cuda-nvjpeg-10-1 cuda-nvjpeg-dev-10-1 cuda-nvrtc-10-1 cuda-nvrtc-dev-10-1\n",
            "  cuda-nvvp-10-1 default-jre dkms freeglut3 freeglut3-dev\n",
            "  keyboard-configuration libargon2-0 libcap2 libcryptsetup12 libcublas10\n",
            "  libdevmapper1.02.1 libfontenc1 libgtk2.0-0 libgtk2.0-common libidn11\n",
            "  libip4tc0 libjansson4 libnvidia-cfg1-440 libnvidia-common-430\n",
            "  libnvidia-common-440 libnvidia-decode-440 libnvidia-encode-440\n",
            "  libnvidia-fbc1-440 libnvidia-gl-440 libnvidia-ifr1-440 libpam-systemd\n",
            "  libpolkit-agent-1-0 libpolkit-backend-1-0 libpolkit-gobject-1-0 libxfont2\n",
            "  libxi-dev libxkbfile1 libxmu-dev libxmu-headers libxnvctrl0\n",
            "  nsight-compute-2019.5.0 nsight-systems-2019.5.2 nvidia-compute-utils-440\n",
            "  nvidia-dkms-440 nvidia-driver-440 nvidia-kernel-common-440\n",
            "  nvidia-kernel-source-440 nvidia-modprobe nvidia-settings nvidia-utils-440\n",
            "  openjdk-11-jre policykit-1 policykit-1-gnome python3-xkit\n",
            "  screen-resolution-extra systemd systemd-sysv udev x11-xkb-utils\n",
            "  xserver-common xserver-xorg-core-hwe-18.04 xserver-xorg-video-nvidia-440\n",
            "Use 'apt autoremove' to remove them.\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension adobe-flashplugin\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 71.9 MB of archives.\n",
            "After this operation, 257 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 78.0.3904.108-0ubuntu0.18.04.1 [1,078 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 78.0.3904.108-0ubuntu0.18.04.1 [63.3 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 78.0.3904.108-0ubuntu0.18.04.1 [3,076 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 78.0.3904.108-0ubuntu0.18.04.1 [4,466 kB]\n",
            "Fetched 71.9 MB in 5s (14.7 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 125048 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_78.0.3904.108-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (78.0.3904.108-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_78.0.3904.108-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (78.0.3904.108-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_78.0.3904.108-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (78.0.3904.108-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_78.0.3904.108-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (78.0.3904.108-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (78.0.3904.108-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (78.0.3904.108-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (78.0.3904.108-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (78.0.3904.108-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhNhB_DtIx-v",
        "colab_type": "text"
      },
      "source": [
        "IMPORTING LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksL0gumnLixq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Libraries to be used for Data Extraction\n",
        "import requests,re\n",
        "from selenium import webdriver\n",
        "from selenium.common.exceptions import NoSuchElementException\n",
        "import lxml\n",
        "from lxml import html,etree\n",
        "import time\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YG-u1Q5qz1m-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Libraries used for Data Analysis\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from collections import Counter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87inW2uQCRES",
        "colab_type": "code",
        "outputId": "496acb38-7a93-4860-b433-33d8f848b36c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "nltk.download('stopwords') \n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjpF3qa2Bb04",
        "colab_type": "code",
        "outputId": "9bc60e4d-938e-4330-f697-58dad0f7ff56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCD2woE9Fb8h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDaiENQNJskD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#adding chrome driver to system path\n",
        "import os,sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "dCKxJLceB6aK",
        "colab_type": "code",
        "outputId": "e5294569-3b97-42b0-9001-d13766ef1433",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "#Configuring Chrome WebDriver\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "#Importing the webdriver\n",
        "driver = webdriver.Chrome('chromedriver',chrome_options=chrome_options)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: use options instead of chrome_options\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29b0bLT3KSxP",
        "colab_type": "text"
      },
      "source": [
        "ALLIED FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKB676ySCh4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#StopWords Preperation\n",
        "rem={\"aren't\",\"below\",\"didn't\",\"didn'\",\"don't\",\"doesn't\",\"doesn'\",\"hadn't\",\"hasn't\",\"haven't\",\"isn't\",\"not\",\"no\",\"wasn't\"}\n",
        "stop_words=set(stopwords.words('english'))\n",
        "stop_words-=rem"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yA8NTmoFgCa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp=spacy.load(\"en\", disable=['ner','parser','tagger'])\n",
        "stopwords_lemma=set([word.lemma_ for word in nlp(' '.join(stop_words))]) #Lemmatizing stopwords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtfOTQQyIHt7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text_2(text):\n",
        "  text=' '.join([word.lemma_ for word in nlp(text)]) #lemmatization\n",
        "  tokenizer = RegexpTokenizer(r'\\w+') #RegularExpression Tokenizer for removing any non-word characters\n",
        "  text=' '.join([w for w in tokenizer.tokenize(text) if not w in stop_words]) #stopwords removal\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQBNs_aKGb2-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Cleaning Review Paragraph Text\n",
        "def clean_text(df,col):\n",
        "  df.iloc[:,col]=df.iloc[:,col].apply(lambda x: x.lower().strip()) #changing to lowercase & stripping whitespaces\n",
        "  df.iloc[:,col]=df.iloc[:,col].apply(lambda x: re.sub('[^a-zA-Z0-9]',' ',x)) #removing characters other than a-z,A-Z,0-9\n",
        "  df.iloc[:,col]=df.iloc[:,col].apply(lambda x: clean_text_2(x)) #lemmatization,removing stopwords and punctuations, keeping only ASCII word characters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "UICzmh9iB6aQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to check if the button is on the page, to avoid miss-click problem\n",
        "def check_exists_by_xpath(xpath):\n",
        "    try:\n",
        "        driver.find_element_by_xpath(xpath)\n",
        "    except NoSuchElementException:\n",
        "        return False\n",
        "    return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqtxct22Mu_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nltk_sent_cal(text):\n",
        "  sid = SentimentIntensityAnalyzer()\n",
        "  return sid.polarity_scores(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvdiEz79KZIR",
        "colab_type": "text"
      },
      "source": [
        "DATA EXTRACTION FUNCTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc0VRbuSnzRR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Functin for Extracting all the restaurant reviews from the TripAdvisor website by using URL and providing Maximum Pages to look\n",
        "def get_review(url,max_page=None):\n",
        "  reviewList=[]\n",
        "  driver.get(url) #Opening URL in webdriver\n",
        "  soup=BeautifulSoup(driver.page_source,'lxml') #Extracting Source Code\n",
        "  page=max_page if max_page else max(map(int,html.fromstring(str(soup)).xpath('//div[contains(@class,\"ui_pagination\")]//a[contains(@class,\"pageNum\")]//text()'))) #Finding maximum number of pages\n",
        "  for _ in range(page):\n",
        "      #expanding reviews for each page\n",
        "      if (check_exists_by_xpath(\"//span[@class='taLnk ulBlueLinks']\")):\n",
        "        driver.find_element_by_xpath(\"//span[@class='taLnk ulBlueLinks']\").click()\n",
        "        time.sleep(3)\n",
        "      soup=BeautifulSoup(driver.page_source,'lxml') #extraction of sourcecode of webpage\n",
        "      review_lists=html.fromstring(str(soup)).xpath('.//div[@id=\"REVIEWS\"]//div[contains(@class,\"review-container\")]') #Xpath for review container\n",
        "      for review in review_lists:\n",
        "        #Xpaths to each of the required data\n",
        "        XPATH_REVIEW_ID='.//div[contains(@class,\"reviewSelector\")]/@data-reviewid'\n",
        "        XPATH_RATING='.//div[contains(@class,\"reviewSelector\")]//span[contains(@class,\"ui_bubble_rating\")]/@class'\n",
        "        XPATH_REVIEW_TITLE='.//div[contains(@class,\"reviewSelector\")]//span[@class=\"noQuotes\"]//text()'\n",
        "        XPATH_REVIEW_DATE='.//div[contains(@class,\"reviewSelector\")]//div[@data-prwidget-name=\"reviews_stay_date_hsx\"]/text()'\n",
        "        XPATH_PARAGRAPH='.//div[contains(@class,\"reviewSelector\")]//div[@data-prwidget-name=\"reviews_text_summary_hsx\"]//p[@class=\"partial_entry\"]//text()'\n",
        "        \n",
        "        #extracting data\n",
        "        raw_review_id=review.xpath(XPATH_REVIEW_ID)\n",
        "        raw_rating=review.xpath(XPATH_RATING)\n",
        "        raw_review_title=review.xpath(XPATH_REVIEW_TITLE)\n",
        "        raw_review_date=review.xpath(XPATH_REVIEW_DATE)\n",
        "        raw_review_paragraph=review.xpath(XPATH_PARAGRAPH)\n",
        "        \n",
        "        #cleaning raw data\n",
        "        ID=raw_review_id[0] if raw_review_id else None\n",
        "        rating=list(raw_rating[0])[-2] if raw_rating else None\n",
        "        title=raw_review_title[0] if raw_review_title else None\n",
        "        date=raw_review_date[0] if raw_review_date else None\n",
        "        paragraph=str(raw_review_paragraph[0]).replace('\\n',' ') if raw_review_paragraph else None\n",
        "        \n",
        "        #storing data\n",
        "        data = {\n",
        "            'Site':url,\n",
        "            #'ID': ID,\n",
        "            'Rating':rating,\n",
        "            'Review Title':title,\n",
        "            'Review Date':date,\n",
        "            'Review Paragraph':paragraph,\n",
        "        }\n",
        "        reviewList.append(data)\n",
        "      \n",
        "      #Moving to Next Page\n",
        "      if (check_exists_by_xpath('*//div[contains(@class,\"ui_pagination\")]//a[@class=\"//span[@class=\"nav next ui_button primary disabled\"]')):\n",
        "        break\n",
        "      if (check_exists_by_xpath('*//div[contains(@class,\"ui_pagination\")]//a[@class=\"nav next taLnk ui_button primary\"]')):\n",
        "        driver.find_element_by_xpath('//div[contains(@class,\"ui_pagination\")]//a[contains(@class,\"nav next taLnk ui_button primary\")]').click()\n",
        "        time.sleep(5)\n",
        "  return reviewList"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibHtcoW_IeHW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creating a list of dictionaries containing all the reviews of all the restaurant from the RESTAURANT_LIST dataframe\n",
        "url='https://www.tripadvisor.in/Restaurant_Review-g304551-d13388460-Reviews-Kitchen_With_A_Cause-New_Delhi_National_Capital_Territory_of_Delhi.html'\n",
        "reviewList=get_review(url)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCtF6v0M3HyQ",
        "colab_type": "text"
      },
      "source": [
        "DATA CLEANING AND ANALYSIS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6Eg9mBOFkzT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Converting the review list into DataFrame\n",
        "DF=pd.DataFrame(reviewList)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Udjd0kUQ763u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Cleaning the review paragraph\n",
        "clean_text(DF,4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHim8Uj7L4jo",
        "colab_type": "code",
        "outputId": "8c394219-5976-461c-bacd-dd55a55ff37c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "#Calculating the Most Frequent Words\n",
        "def most_freq(count):\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  tokens = tokenizer.tokenize((\" \".join(DF[\"Review Paragraph\"])))\n",
        "  word_count=Counter(tokens)\n",
        "  return word_count.most_common(count)\n",
        "print (\"Top Tem Most Frequent Words: \")\n",
        "for i,j in most_freq(10):\n",
        "  print (\"Word - %s : Occurance - %s\"%(i,j))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top Tem Most Frequent Words: \n",
            "Word - food : Occurance - 183\n",
            "Word - great : Occurance - 100\n",
            "Word - restaurant : Occurance - 100\n",
            "Word - good : Occurance - 95\n",
            "Word - place : Occurance - 71\n",
            "Word - staff : Occurance - 63\n",
            "Word - cause : Occurance - 57\n",
            "Word - service : Occurance - 56\n",
            "Word - visit : Occurance - 52\n",
            "Word - delhi : Occurance - 51\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EADh55C3qo3R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Copy for Data Analysis\n",
        "nltk_DF=DF.copy()\n",
        "clean_text(nltk_DF,2)\n",
        "nltk_DF.iloc[:,4]=DF.iloc[:,2]+':'+DF.iloc[:,4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohJtod4sNGbq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Calculation of Polarity Score\n",
        "nltk_DF['POLARITY']=nltk_DF[\"Review Paragraph\"].apply(lambda x: nltk_sent_cal(x)['compound'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFyygTnhnExp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "1e437972-c00d-4877-caff-1c4e6354d4c1"
      },
      "source": [
        "print (\"MOST POSITIVE REVIEW\")\n",
        "pos_index=nltk_DF[nltk_DF.POLARITY==max(nltk_DF.POLARITY)].index[0]\n",
        "print (\"Review Polarity - %s\"%nltk_DF[\"POLARITY\"][pos_index])\n",
        "print (\"Review Title - %s\"%DF[\"Review Title\"][pos_index])\n",
        "print (\"Review Paragraph - %s\"%DF[\"Review Paragraph\"][pos_index])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MOST POSITIVE REVIEW\n",
            "Review Polarity - 0.9955\n",
            "Review Title - Such a beautiful experience and food, for such a great cause\n",
            "Review Paragraph - mum go restaurant twice 2 separate stay delhi precisely love much find food reasonably price super tasty staff totally wonderful offer variety food choose love super trendy relax restaurant lovely atmosphere really like know ethos behind restaurant train teenager street child progress far super recommend not disappoint attach couple photo take place wonderful food justice\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_L7w29ABncoS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "9fd0255d-ff8d-4280-852a-e52d2c9f004d"
      },
      "source": [
        "print (\"MOST NEGATIVE REVIEW\")\n",
        "neg_index=nltk_DF[nltk_DF.POLARITY==min(nltk_DF.POLARITY)].index[0]\n",
        "print (\"Review Polarity - %s\"%nltk_DF[\"POLARITY\"][neg_index])\n",
        "print (\"Review Title - %s\"%DF[\"Review Title\"][neg_index])\n",
        "print (\"Review Paragraph - %s\"%DF[\"Review Paragraph\"][neg_index])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MOST NEGATIVE REVIEW\n",
            "Review Polarity - -0.7011\n",
            "Review Title - Kitchen without a cause\n",
            "Review Paragraph - 5 star review restaurant give people cause concern like many westerner review restaurant also go dinner think place good food take age arrive overpriced not local standard much like tesco package food western palate use especially shahi paneer full sugar red colour no cashew paste traditional spice ghee grow locally know authentic food taste like not pay not eat take convince hakka noodle okay though paneer order give son stomach problem morning ambience poor bad acoustic timber palette hang ceil obviously designer restaurant see trendy western restaurant case specially make architect though layout poor family get drink make lot noise manager timid stop much worryingly not get proper answer charitable cause tell support street kid not see charity registration numb not see street kid 1 intern give interview facebook page give detail learn appear street kid good educate adult talk street kind area never hear restaurant neither local business association never go instead visit authentic south indian restaurant next door also see kwac staff daily watch horde naive western people go think help poor eat terrible overpriced food may well give money register charity eat authentic food somewhere else instead\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKXSHDaXnhxr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "0b693645-f70b-43d7-d55d-cf919e4927bb"
      },
      "source": [
        "print (\"NEUTRAL REVIEWS\")\n",
        "neu_index=nltk_DF[nltk_DF.POLARITY==0].index[0]\n",
        "print (\"Review Polarity - %s\"%nltk_DF[\"POLARITY\"][neu_index])\n",
        "print (\"Review Title - %s\"%DF[\"Review Title\"][neu_index])\n",
        "print (\"Review Paragraph - %s\"%DF[\"Review Paragraph\"][neu_index])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NEUTRAL REVIEWS\n",
            "Review Polarity - 0.0\n",
            "Review Title - Very tasty food\n",
            "Review Paragraph - spend arround week india go meal time restaurant far one much tasty indian food come across staff professional polite\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BanD6Reenkxs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Percentage of Positive and Negative Reviews\n",
        "positive_percent=nltk_DF[nltk_DF.POLARITY>0].POLARITY.count()/nltk_DF.POLARITY.count()*100\n",
        "negative_percent=nltk_DF[nltk_DF.POLARITY<0].POLARITY.count()/nltk_DF.POLARITY.count()*100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7uzC0LRn2Sj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "65db1d31-9eeb-4ab3-c870-bb99e420f41b"
      },
      "source": [
        "print (\"Positive Reviews : %f\"%positive_percent,'%')\n",
        "print (\"Negative Reviews : %f\"%negative_percent,'%')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive Reviews : 97.727273 %\n",
            "Negative Reviews : 1.704545 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jm0CkKr2MFXW",
        "colab": {}
      },
      "source": [
        "#Converting th review file into CSV\n",
        "DF.to_csv('Data Analyst Internship Submission.csv',header=True,index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}